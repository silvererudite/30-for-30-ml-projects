{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport torch\nimport torchvision\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom torchvision import utils, datasets, transforms\nfrom torch.utils.data import random_split\nimport torch.nn as nn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-30T12:24:50.172801Z","iopub.execute_input":"2022-12-30T12:24:50.173209Z","iopub.status.idle":"2022-12-30T12:24:50.180220Z","shell.execute_reply.started":"2022-12-30T12:24:50.173175Z","shell.execute_reply":"2022-12-30T12:24:50.178839Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/shark-species/sharks/'","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:24:50.182277Z","iopub.execute_input":"2022-12-30T12:24:50.182708Z","iopub.status.idle":"2022-12-30T12:24:50.192142Z","shell.execute_reply.started":"2022-12-30T12:24:50.182667Z","shell.execute_reply":"2022-12-30T12:24:50.191206Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"class_list = os.listdir(DATA_DIR)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:24:50.194182Z","iopub.execute_input":"2022-12-30T12:24:50.194570Z","iopub.status.idle":"2022-12-30T12:24:50.208038Z","shell.execute_reply.started":"2022-12-30T12:24:50.194530Z","shell.execute_reply":"2022-12-30T12:24:50.206605Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"#np.random.seed(42)\n# fig = plt.figure(figsize=(10, 5))\n# selected_class = np.random.choice(class_list)\n# selected_files = os.listdir(DATA_DIR+f'{selected_class}')\n\n# for i, file in enumerate(selected_files[5:11]):\n#   img = Image.open(DATA_DIR+f'{selected_class}/'+file)\n#   #print('Image shape: ', np.array(img).shape)\n#   ax = fig.add_subplot(2, 3, i+1)\n#   ax.set_xticks([])\n#   ax.set_yticks([])\n#   ax.imshow(img)\n#   ax.set_title(f'{selected_class}', size=15)\n# plt.tight_layout()\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:24:50.209925Z","iopub.execute_input":"2022-12-30T12:24:50.210753Z","iopub.status.idle":"2022-12-30T12:24:50.218317Z","shell.execute_reply.started":"2022-12-30T12:24:50.210705Z","shell.execute_reply":"2022-12-30T12:24:50.217465Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"def readImg(path):\n    img = Image.open(path)\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:24:50.220104Z","iopub.execute_input":"2022-12-30T12:24:50.221127Z","iopub.status.idle":"2022-12-30T12:24:50.236081Z","shell.execute_reply.started":"2022-12-30T12:24:50.221091Z","shell.execute_reply":"2022-12-30T12:24:50.234887Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([transforms.Resize((128,128),interpolation=Image.Resampling.NEAREST),\n                                transforms.RandomHorizontalFlip(p=0.5),\n                                transforms.Grayscale(),\n                                 transforms.ToTensor()])","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:24:50.238109Z","iopub.execute_input":"2022-12-30T12:24:50.238863Z","iopub.status.idle":"2022-12-30T12:24:50.251383Z","shell.execute_reply.started":"2022-12-30T12:24:50.238814Z","shell.execute_reply":"2022-12-30T12:24:50.249977Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset = datasets.ImageFolder(DATA_DIR, transform=transform, loader=readImg)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:24:50.253445Z","iopub.execute_input":"2022-12-30T12:24:50.253918Z","iopub.status.idle":"2022-12-30T12:24:50.284983Z","shell.execute_reply.started":"2022-12-30T12:24:50.253874Z","shell.execute_reply":"2022-12-30T12:24:50.283755Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"dataloader = torch.utils.data.DataLoader(dataset, batch_size=128)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:24:50.287545Z","iopub.execute_input":"2022-12-30T12:24:50.290315Z","iopub.status.idle":"2022-12-30T12:24:50.296564Z","shell.execute_reply.started":"2022-12-30T12:24:50.290272Z","shell.execute_reply":"2022-12-30T12:24:50.295313Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"def imshow(image, ax=None, title=None, normalize=True):\n    \"\"\"Imshow for Tensor.\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    image = image.numpy().transpose((1, 2, 0))\n\n    if normalize:\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = std * image + mean\n        image = np.clip(image, 0, 1)\n\n    ax.imshow(image)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n\n    return ax","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:24:50.297984Z","iopub.execute_input":"2022-12-30T12:24:50.298877Z","iopub.status.idle":"2022-12-30T12:24:50.310046Z","shell.execute_reply.started":"2022-12-30T12:24:50.298840Z","shell.execute_reply":"2022-12-30T12:24:50.308828Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# images, labels = next(iter(dataloader))\n# # helper.imshow(images[0], normalize=False)\n# imshow(images[0], normalize=False)\n# print(labels[0])","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:24:50.314491Z","iopub.execute_input":"2022-12-30T12:24:50.315120Z","iopub.status.idle":"2022-12-30T12:24:50.323125Z","shell.execute_reply.started":"2022-12-30T12:24:50.315068Z","shell.execute_reply":"2022-12-30T12:24:50.321923Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"# for img, label in dataloader:\n#     print(label)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:24:50.325049Z","iopub.execute_input":"2022-12-30T12:24:50.325507Z","iopub.status.idle":"2022-12-30T12:24:50.335363Z","shell.execute_reply.started":"2022-12-30T12:24:50.325463Z","shell.execute_reply":"2022-12-30T12:24:50.334336Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"dataset.class_to_idx","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:24:50.337150Z","iopub.execute_input":"2022-12-30T12:24:50.337864Z","iopub.status.idle":"2022-12-30T12:24:50.352738Z","shell.execute_reply.started":"2022-12-30T12:24:50.337828Z","shell.execute_reply":"2022-12-30T12:24:50.351831Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"{'basking': 0,\n 'blacktip': 1,\n 'blue': 2,\n 'bull': 3,\n 'hammerhead': 4,\n 'lemon': 5,\n 'mako': 6,\n 'nurse': 7,\n 'sand tiger': 8,\n 'thresher': 9,\n 'tiger': 10,\n 'whale': 11,\n 'white': 12,\n 'whitetip': 13}"},"metadata":{}}]},{"cell_type":"code","source":"len_img=len(dataset)\nlen_train=int(0.8*len_img)\nlen_val=len_img-len_train\n\n# Split Pytorch tensor\ntrain_dl,val_dl=random_split(dataset,\n                             [len_train,len_val]) # random split 80/20\n\nprint(\"train dataset size:\", len(train_dl))\nprint(\"validation dataset size:\", len(val_dl))","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:24:50.354464Z","iopub.execute_input":"2022-12-30T12:24:50.355205Z","iopub.status.idle":"2022-12-30T12:24:50.368950Z","shell.execute_reply.started":"2022-12-30T12:24:50.355170Z","shell.execute_reply":"2022-12-30T12:24:50.367623Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"train dataset size: 1236\nvalidation dataset size: 310\n","output_type":"stream"}]},{"cell_type":"code","source":"class CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n        \n        # Convolution 1\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n        self.relu1 = nn.ReLU()\n        \n        # Max pool 1\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n     \n        # Convolution 2\n        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n        self.relu2 = nn.ReLU()\n        \n        # Max pool 2\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n        \n        # Convolution 3\n        self.cnn3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=0)\n        self.relu3 = nn.ReLU()\n        \n        # Max pool 3\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n        \n        # Convolution 4\n        self.cnn4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=0)\n        self.relu4 = nn.ReLU()\n        \n        # Max pool 3\n        self.maxpool4 = nn.MaxPool2d(kernel_size=2)\n        \n        # Fully connected 1\n        self.fc1 = nn.Linear(128*4*4, 14) \n    \n    def forward(self, x):\n        # Convolution 1\n        out = self.cnn1(x)\n        out = self.relu1(out)\n        \n        # Max pool 1\n        out = self.maxpool1(out)\n        \n        # Convolution 2 \n        out = self.cnn2(out)\n        out = self.relu2(out)\n        \n        # Max pool 2 \n        out = self.maxpool2(out)\n        \n        # Convolution 3 \n        out = self.cnn3(out)\n        out = self.relu3(out)\n        \n        # Max pool 3 \n        out = self.maxpool3(out)\n        \n        # Convolution 4 \n        out = self.cnn4(out)\n        out = self.relu4(out)\n        \n        # Max pool 2 \n        out = self.maxpool4(out)\n        \n        # flatten\n        out = out.view(out.size(0), -1)\n\n        # Linear function (readout)\n        out = self.fc1(out)\n        \n        return out\n\n# batch_size, epoch and iteration\nbatch_size = 128\nn_iters = 25\nnum_epochs = 30\n\n\n# data loader\ntrain_loader = torch.utils.data.DataLoader(train_dl, batch_size = batch_size, shuffle = True)\ntest_loader = torch.utils.data.DataLoader(val_dl, batch_size = batch_size, shuffle = True)\n    \n# Create CNN\nmodel = CNNModel()\n\n# Cross Entropy Loss \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlearning_rate = 0.1\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:24:50.370330Z","iopub.execute_input":"2022-12-30T12:24:50.370868Z","iopub.status.idle":"2022-12-30T12:24:50.394104Z","shell.execute_reply.started":"2022-12-30T12:24:50.370835Z","shell.execute_reply":"2022-12-30T12:24:50.392984Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"for img, label in train_loader:\n    print(img.shape)\n    print(label.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:24:50.395657Z","iopub.execute_input":"2022-12-30T12:24:50.396629Z","iopub.status.idle":"2022-12-30T12:24:55.685706Z","shell.execute_reply.started":"2022-12-30T12:24:50.396586Z","shell.execute_reply":"2022-12-30T12:24:55.684561Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"torch.Size([128, 1, 128, 128])\ntorch.Size([128])\n","output_type":"stream"}]},{"cell_type":"code","source":"model.parameters","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:24:55.687434Z","iopub.execute_input":"2022-12-30T12:24:55.687833Z","iopub.status.idle":"2022-12-30T12:24:55.694399Z","shell.execute_reply.started":"2022-12-30T12:24:55.687799Z","shell.execute_reply":"2022-12-30T12:24:55.693487Z"},"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"<bound method Module.parameters of CNNModel(\n  (cnn1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (relu1): ReLU()\n  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (cnn2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n  (relu2): ReLU()\n  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (cnn3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n  (relu3): ReLU()\n  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (cnn4): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n  (relu4): ReLU()\n  (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=2048, out_features=14, bias=True)\n)>"},"metadata":{}}]},{"cell_type":"code","source":"import torch.nn.functional as F\ndef calc_accuracy(true,pred):\n    pred = F.softmax(pred, dim = 1)\n    true = torch.zeros(pred.shape[0], pred.shape[1]).scatter_(1, true.unsqueeze(1), 1.)\n    acc = (true.argmax(-1) == pred.argmax(-1)).float().detach().numpy()\n    acc = float((100 * acc.sum()) / len(acc))\n    return round(acc, 4)\n# def calc_accuracy(mdl, X, Y):\n#     # reduce/collapse the classification dimension according to max op\n#     # resulting in most likely label\n#     max_vals, max_indices = mdl(X).max(1)\n#     # assumes the first dimension is batch size\n#     n = max_indices.size(0)  # index 0 for extracting the # of elements\n#     # calulate acc (note .item() to do float division)\n#     acc = (max_indices == Y).sum().item() / n\n#     return acc","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:24:55.695749Z","iopub.execute_input":"2022-12-30T12:24:55.696265Z","iopub.status.idle":"2022-12-30T12:24:55.716194Z","shell.execute_reply.started":"2022-12-30T12:24:55.696234Z","shell.execute_reply":"2022-12-30T12:24:55.714980Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:24:55.717632Z","iopub.execute_input":"2022-12-30T12:24:55.717978Z","iopub.status.idle":"2022-12-30T12:24:55.729247Z","shell.execute_reply.started":"2022-12-30T12:24:55.717947Z","shell.execute_reply":"2022-12-30T12:24:55.728053Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"import time\ntrain_loss = []\ntrain_accuracy = []\n\nval_loss = []\nval_accuracy = []\n\nfor epoch in range(num_epochs):\n    \n    start = time.time()\n    \n    #Epoch Loss & Accuracy\n    train_epoch_loss = []\n    train_epoch_accuracy = []\n    _iter = 1\n    \n    #Val Loss & Accuracy\n    val_epoch_loss = []\n    val_epoch_accuracy = []\n    \n    # Training\n    for images, labels in train_loader:\n        \n        images = images.to(device)\n        labels = labels.to(device)\n\n#         print(images.shape)\n#         print(labels.shape)\n        #Reset Grads\n        optimizer.zero_grad()\n        \n        #Forward ->\n        preds = model(images)\n        \n        #Calculate Accuracy\n        acc = calc_accuracy(labels.cpu(), preds.cpu())\n        \n        #Calculate Loss & Backward, Update Weights (Step)\n        loss = error(preds, labels)\n        loss.backward()\n        optimizer.step()\n        \n        #Append loss & acc\n        loss_value = loss.item()\n        train_epoch_loss.append(loss_value)\n        train_epoch_accuracy.append(acc)\n        \n        if _iter % 500 == 0:\n            print(\"> Iteration {} < \".format(_iter))\n            print(\"Iter Loss = {}\".format(round(loss_value, 4)))\n            print(\"Iter Accuracy = {} % \\n\".format(acc))\n        \n        _iter += 1\n    \n    #Validation\n    for images, labels in test_loader:\n        \n        images = images.to(device)\n        labels = labels.to(device)\n       \n        #Forward ->\n        preds = model(images)\n        \n        #Calculate Accuracy\n        acc = calc_accuracy(labels.cpu(), preds.cpu())\n        \n        #Calculate Loss\n        loss = error(preds, labels)\n        \n        #Append loss & acc\n        loss_value = loss.item()\n        val_epoch_loss.append(loss_value)\n        val_epoch_accuracy.append(acc)\n    \n    \n    train_epoch_loss = np.mean(train_epoch_loss)\n    train_epoch_accuracy = np.mean(train_epoch_accuracy)\n    \n    val_epoch_loss = np.mean(val_epoch_loss)\n    val_epoch_accuracy = np.mean(val_epoch_accuracy)\n    \n    end = time.time()\n    \n    train_loss.append(train_epoch_loss)\n    train_accuracy.append(train_epoch_accuracy)\n    \n    val_loss.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)\n    \n    #Print Epoch Statistics\n    print(\"** Epoch {} ** - Epoch Time {}\".format(epoch, int(end-start)))\n    print(\"Train Loss = {}\".format(round(train_epoch_loss, 4)))\n    print(\"Train Accuracy = {} %....Val Accuracy = {} % \\n\".format(train_epoch_accuracy, val_epoch_accuracy))\n    print(\"Val Loss = {}\".format(np.round(val_epoch_loss, 3)))\n    print(\"Val Accuracy = {} % \\n\".format(np.round(val_epoch_accuracy, 3)))","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:24:55.731004Z","iopub.execute_input":"2022-12-30T12:24:55.732101Z","iopub.status.idle":"2022-12-30T12:59:45.772298Z","shell.execute_reply.started":"2022-12-30T12:24:55.732061Z","shell.execute_reply":"2022-12-30T12:59:45.771370Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":81,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/PIL/Image.py:993: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n","output_type":"stream"},{"name":"stdout","text":"** Epoch 0 ** - Epoch Time 69\nTrain Loss = 2.6372\nTrain Accuracy = 7.51488 %....Val Accuracy = 9.239966666666666 % \n\nVal Loss = 2.636\nVal Accuracy = 9.24 % \n\n** Epoch 1 ** - Epoch Time 69\nTrain Loss = 2.6319\nTrain Accuracy = 9.27456 %....Val Accuracy = 8.294766666666666 % \n\nVal Loss = 2.637\nVal Accuracy = 8.295 % \n\n** Epoch 2 ** - Epoch Time 69\nTrain Loss = 2.6263\nTrain Accuracy = 8.84301 %....Val Accuracy = 7.5810666666666675 % \n\nVal Loss = 2.64\nVal Accuracy = 7.581 % \n\n** Epoch 3 ** - Epoch Time 69\nTrain Loss = 2.6233\nTrain Accuracy = 9.03274 %....Val Accuracy = 7.9378666666666655 % \n\nVal Loss = 2.632\nVal Accuracy = 7.938 % \n\n** Epoch 4 ** - Epoch Time 69\nTrain Loss = 2.6181\nTrain Accuracy = 9.78426 %....Val Accuracy = 7.677466666666667 % \n\nVal Loss = 2.648\nVal Accuracy = 7.677 % \n\n** Epoch 5 ** - Epoch Time 69\nTrain Loss = 2.6142\nTrain Accuracy = 8.51934 %....Val Accuracy = 8.458699999999999 % \n\nVal Loss = 2.619\nVal Accuracy = 8.459 % \n\n** Epoch 6 ** - Epoch Time 69\nTrain Loss = 2.5972\nTrain Accuracy = 11.577369999999998 %....Val Accuracy = 10.3781 % \n\nVal Loss = 2.596\nVal Accuracy = 10.378 % \n\n** Epoch 7 ** - Epoch Time 69\nTrain Loss = 2.567\nTrain Accuracy = 12.18005 %....Val Accuracy = 8.1983 % \n\nVal Loss = 2.599\nVal Accuracy = 8.198 % \n\n** Epoch 8 ** - Epoch Time 70\nTrain Loss = 2.5425\nTrain Accuracy = 12.08334 %....Val Accuracy = 11.583733333333335 % \n\nVal Loss = 2.556\nVal Accuracy = 11.584 % \n\n** Epoch 9 ** - Epoch Time 69\nTrain Loss = 2.4945\nTrain Accuracy = 14.360130000000002 %....Val Accuracy = 9.239966666666666 % \n\nVal Loss = 2.615\nVal Accuracy = 9.24 % \n\n** Epoch 10 ** - Epoch Time 70\nTrain Loss = 2.5552\nTrain Accuracy = 13.385399999999999 %....Val Accuracy = 12.364933333333333 % \n\nVal Loss = 2.585\nVal Accuracy = 12.365 % \n\n** Epoch 11 ** - Epoch Time 69\nTrain Loss = 2.5065\nTrain Accuracy = 15.61757 %....Val Accuracy = 14.052866666666667 % \n\nVal Loss = 2.564\nVal Accuracy = 14.053 % \n\n** Epoch 12 ** - Epoch Time 69\nTrain Loss = 2.462\nTrain Accuracy = 17.71949 %....Val Accuracy = 11.969499999999998 % \n\nVal Loss = 2.61\nVal Accuracy = 11.969 % \n\n** Epoch 13 ** - Epoch Time 70\nTrain Loss = 2.4239\nTrain Accuracy = 18.74257 %....Val Accuracy = 14.313266666666665 % \n\nVal Loss = 2.537\nVal Accuracy = 14.313 % \n\n** Epoch 14 ** - Epoch Time 69\nTrain Loss = 2.4321\nTrain Accuracy = 18.195700000000002 %....Val Accuracy = 15.943266666666666 % \n\nVal Loss = 2.571\nVal Accuracy = 15.943 % \n\n** Epoch 15 ** - Epoch Time 69\nTrain Loss = 2.3971\nTrain Accuracy = 19.33406 %....Val Accuracy = 10.706066666666667 % \n\nVal Loss = 2.55\nVal Accuracy = 10.706 % \n\n** Epoch 16 ** - Epoch Time 69\nTrain Loss = 2.408\nTrain Accuracy = 19.95161 %....Val Accuracy = 16.7535 % \n\nVal Loss = 2.534\nVal Accuracy = 16.754 % \n\n** Epoch 17 ** - Epoch Time 69\nTrain Loss = 2.4021\nTrain Accuracy = 19.24108 %....Val Accuracy = 13.6285 % \n\nVal Loss = 2.537\nVal Accuracy = 13.628 % \n\n** Epoch 18 ** - Epoch Time 69\nTrain Loss = 2.3503\nTrain Accuracy = 22.250729999999997 %....Val Accuracy = 13.2716 % \n\nVal Loss = 2.537\nVal Accuracy = 13.272 % \n\n** Epoch 19 ** - Epoch Time 69\nTrain Loss = 2.3767\nTrain Accuracy = 20.502239999999997 %....Val Accuracy = 18.373833333333334 % \n\nVal Loss = 2.493\nVal Accuracy = 18.374 % \n\n** Epoch 20 ** - Epoch Time 69\nTrain Loss = 2.3665\nTrain Accuracy = 22.29538 %....Val Accuracy = 17.37076666666667 % \n\nVal Loss = 2.482\nVal Accuracy = 17.371 % \n\n** Epoch 21 ** - Epoch Time 69\nTrain Loss = 2.3077\nTrain Accuracy = 23.90624 %....Val Accuracy = 14.573666666666666 % \n\nVal Loss = 2.523\nVal Accuracy = 14.574 % \n\n** Epoch 22 ** - Epoch Time 69\nTrain Loss = 2.3291\nTrain Accuracy = 24.53124 %....Val Accuracy = 16.136166666666664 % \n\nVal Loss = 2.525\nVal Accuracy = 16.136 % \n\n** Epoch 23 ** - Epoch Time 70\nTrain Loss = 2.291\nTrain Accuracy = 26.28719 %....Val Accuracy = 16.10723333333333 % \n\nVal Loss = 2.496\nVal Accuracy = 16.107 % \n\n** Epoch 24 ** - Epoch Time 70\nTrain Loss = 2.2912\nTrain Accuracy = 25.0744 %....Val Accuracy = 17.0139 % \n\nVal Loss = 2.53\nVal Accuracy = 17.014 % \n\n** Epoch 25 ** - Epoch Time 70\nTrain Loss = 2.2252\nTrain Accuracy = 28.441219999999998 %....Val Accuracy = 19.290133333333333 % \n\nVal Loss = 2.491\nVal Accuracy = 19.29 % \n\n** Epoch 26 ** - Epoch Time 69\nTrain Loss = 2.3815\nTrain Accuracy = 23.20685 %....Val Accuracy = 17.534699999999997 % \n\nVal Loss = 2.478\nVal Accuracy = 17.535 % \n\n** Epoch 27 ** - Epoch Time 69\nTrain Loss = 2.28\nTrain Accuracy = 25.77382 %....Val Accuracy = 11.5837 % \n\nVal Loss = 2.569\nVal Accuracy = 11.584 % \n\n** Epoch 28 ** - Epoch Time 68\nTrain Loss = 2.2757\nTrain Accuracy = 27.172610000000002 %....Val Accuracy = 16.2037 % \n\nVal Loss = 2.464\nVal Accuracy = 16.204 % \n\n** Epoch 29 ** - Epoch Time 68\nTrain Loss = 2.2479\nTrain Accuracy = 26.79686 %....Val Accuracy = 16.03976666666667 % \n\nVal Loss = 2.461\nVal Accuracy = 16.04 % \n\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model, '/kaggle/working/')","metadata":{"execution":{"iopub.status.busy":"2022-12-30T13:09:33.417749Z","iopub.execute_input":"2022-12-30T13:09:33.418142Z","iopub.status.idle":"2022-12-30T13:09:33.440212Z","shell.execute_reply.started":"2022-12-30T13:09:33.418111Z","shell.execute_reply":"2022-12-30T13:09:33.438891Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":85,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/291968944.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/kaggle/working/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/kaggle/working/'"],"ename":"IsADirectoryError","evalue":"[Errno 21] Is a directory: '/kaggle/working/'","output_type":"error"}]},{"cell_type":"code","source":"model_scripted = torch.jit.script(model) # Export to TorchScript\nmodel_scripted.save('model_scripted.pt') # Save","metadata":{"execution":{"iopub.status.busy":"2022-12-30T13:12:46.235663Z","iopub.execute_input":"2022-12-30T13:12:46.236075Z","iopub.status.idle":"2022-12-30T13:12:46.299115Z","shell.execute_reply.started":"2022-12-30T13:12:46.236042Z","shell.execute_reply":"2022-12-30T13:12:46.297808Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}