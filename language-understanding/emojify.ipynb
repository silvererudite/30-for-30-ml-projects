{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import emoji\n",
    "from emo_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "from test_utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = read_csv('data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv('data/tesss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = len(max(X_train, key=len).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_emoji(label):\n",
    "    \"\"\"\n",
    "    Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n",
    "    \"\"\"\n",
    "    return emoji.emojize(emoji_dictionary[str(label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never talk to me again :disappointed:\n",
      "I am proud of your achievements :smile:\n",
      "It is the worst day in my life :disappointed:\n",
      "Miss you so much ❤️\n",
      "food is life 🍴\n",
      "I love you mum ❤️\n",
      "Stop saying bullshit :disappointed:\n",
      "congratulations on your acceptance :smile:\n",
      "The assignment is too long  :disappointed:\n",
      "I want to go play ⚾\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(X_train[idx], label_to_emoji(Y_train[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 'Stop shouting at me' has label index 3, which is emoji :disappointed:\n",
      "Label index 3 in one-hot encoding format is [0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "idx = 15\n",
    "print(f\"Sentence '{X_train[15]}' has label index {Y_train[idx]}, which is emoji {label_to_emoji(Y_train[idx])}\", )\n",
    "print(f\"Label index {Y_train[idx]} in one-hot encoding format is {Y_oh_train[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "word_to_vec_map = {}\n",
    "f = open('data/glove.6B.50d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "  line = line.strip().split()\n",
    "  curr_word = line[0]\n",
    "  words.add(curr_word)\n",
    "  word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "\n",
    "i = 1\n",
    "words_to_index = {}\n",
    "index_to_words = {}\n",
    "for w in sorted(words):\n",
    "    words_to_index[w] = i\n",
    "    index_to_words[i] = w\n",
    "    i = i + 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of cucumber in the vocabulary is 113317\n",
      "the 289846th word in the vocabulary is potatos\n"
     ]
    }
   ],
   "source": [
    "word = \"cucumber\"\n",
    "idx = 289846\n",
    "print(\"the index of\", word, \"in the vocabulary is\", words_to_index[word])\n",
    "print(\"the\", str(idx) + \"th word in the vocabulary is\", index_to_words[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word\n",
    "    and averages its value into a single vector encoding the meaning of the sentence.\n",
    "    \n",
    "    Arguments:\n",
    "    sentence -- string, one training example from X\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    \n",
    "    Returns:\n",
    "    avg -- average vector encoding information about the sentence, numpy-array of shape (50,)\n",
    "    \"\"\"\n",
    "    any_word = list(word_to_vec_map.keys())[0]\n",
    "    words = sentence.lower().split()\n",
    "\n",
    "    avg = np.zeros(word_to_vec_map[any_word].shape)\n",
    "    count = 0\n",
    "\n",
    "    for word in words:\n",
    "      if word in list(word_to_vec_map.keys()):\n",
    "        avg += word_to_vec_map[word]\n",
    "        # Increment count\n",
    "        count +=1\n",
    "    if count > 0:\n",
    "        # Get the average. But only if count > 0\n",
    "        avg = avg/count\n",
    "\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = \n",
      " [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
      " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
      "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
      "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
      "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
      "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
      " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
      " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
      "  0.1445417   0.09808667]\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
    "print(\"avg = \\n\", avg)\n",
    "\n",
    "def sentence_to_avg_test(target):\n",
    "    # Create a controlled word to vec map\n",
    "    word_to_vec_map = {'a': [3, 3], 'synonym_of_a': [3, 3], 'a_nw': [2, 4], 'a_s': [3, 2], \n",
    "                       'c': [-2, 1], 'c_n': [-2, 2],'c_ne': [-1, 2], 'c_e': [-1, 1], 'c_se': [-1, 0], \n",
    "                       'c_s': [-2, 0], 'c_sw': [-3, 0], 'c_w': [-3, 1], 'c_nw': [-3, 2]\n",
    "                      }\n",
    "    # Convert lists to np.arrays\n",
    "    for key in word_to_vec_map.keys():\n",
    "        word_to_vec_map[key] = np.array(word_to_vec_map[key])\n",
    "        \n",
    "    avg = target(\"a a_nw c_w a_s\", word_to_vec_map)\n",
    "    assert tuple(avg.shape) == tuple(word_to_vec_map['a'].shape),  \"Check the shape of your avg array\"  \n",
    "    assert np.allclose(avg, [1.25, 2.5]),  \"Check that you are finding the 4 words\"\n",
    "    avg = target(\"love a a_nw c_w a_s\", word_to_vec_map)\n",
    "    assert np.allclose(avg, [1.25, 2.5]), \"Divide by count, not len(words)\"\n",
    "    avg = target(\"love\", word_to_vec_map)\n",
    "    assert np.allclose(avg, [0, 0]), \"Average of no words must give an array of zeros\"\n",
    "    avg = target(\"c_se foo a a_nw c_w a_s deeplearning c_nw\", word_to_vec_map)\n",
    "    assert np.allclose(avg, [0.1666667, 2.0]), \"Debug the last example\"\n",
    "    \n",
    "    print(\"\\033[92mAll tests passed!\")\n",
    "    \n",
    "sentence_to_avg_test(sentence_to_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 20):\n",
    "    \"\"\"\n",
    "    Model to train word vector representations in numpy.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, numpy array of sentences as strings, of shape (m, 1)\n",
    "    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    learning_rate -- learning_rate for the stochastic gradient descent algorithm\n",
    "    num_iterations -- number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    pred -- vector of predictions, numpy-array of shape (m, 1)\n",
    "    W -- weight matrix of the softmax layer, of shape (n_y, n_h)\n",
    "    b -- bias of the softmax layer, of shape (n_y,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get a valid word contained in the word_to_vec_map \n",
    "    any_word = list(word_to_vec_map.keys())[0]\n",
    "        \n",
    "    # Initialize cost. It is needed during grading\n",
    "    cost = 0\n",
    "    # Define number of training examples\n",
    "    m = Y.shape[0]                             # number of training examples\n",
    "    n_y = len(np.unique(Y))                    # number of classes  \n",
    "    n_h = word_to_vec_map[any_word].shape[0]   # dimensions of the GloVe vectors \n",
    "    \n",
    "    # Initialize parameters using Xavier initialization\n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    # Convert Y to Y_onehot with n_y classes\n",
    "    Y_oh = convert_to_one_hot(Y, C = n_y) \n",
    "    \n",
    "    # Optimization loop\n",
    "    for t in range(num_iterations): # Loop over the number of iterations\n",
    "        for i in range(m):          # Loop over the training examples\n",
    "          avg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "          z = np.dot(W, avg) + b\n",
    "          a = softmax(z)\n",
    "          cost = -np.sum(np.dot(Y_oh[i], np.log(a)))\n",
    "\n",
    "          dz = a - Y_oh[i]\n",
    "          dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "          db = dz\n",
    "\n",
    "          W = W - learning_rate * dW\n",
    "          b = b - learning_rate * db\n",
    "        if t % 10 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y, W, b, word_to_vec_map) #predict is defined in emo_utils.py\n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 0.05105772513207823\n",
      "Accuracy: 0.9166666666666666\n",
      "Epoch: 10 --- cost = 0.03893716809550303\n",
      "Accuracy: 0.9166666666666666\n",
      "Epoch: 20 --- cost = 0.03078199487865477\n",
      "Accuracy: 0.9166666666666666\n",
      "Epoch: 30 --- cost = 0.0250625254818359\n",
      "Accuracy: 0.9166666666666666\n",
      "Epoch: 40 --- cost = 0.020906296817970453\n",
      "Accuracy: 0.9166666666666666\n",
      "Epoch: 50 --- cost = 0.017791876552201644\n",
      "Accuracy: 0.9166666666666666\n",
      "Epoch: 60 --- cost = 0.015394747683241103\n",
      "Accuracy: 0.9166666666666666\n",
      "Epoch: 70 --- cost = 0.01350575460904451\n",
      "Accuracy: 1.0\n",
      "Epoch: 80 --- cost = 0.01198593465621373\n",
      "Accuracy: 1.0\n",
      "Epoch: 90 --- cost = 0.01074046826626526\n",
      "Accuracy: 1.0\n",
      "Epoch: 100 --- cost = 0.00970311068897676\n",
      "Accuracy: 1.0\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "def model_test(target):\n",
    "    # Create a controlled word to vec map\n",
    "    word_to_vec_map = {'a': [3, 3], 'synonym_of_a': [3, 3], 'a_nw': [2, 4], 'a_s': [3, 2], 'a_n': [3, 4], \n",
    "                       'c': [-2, 1], 'c_n': [-2, 2],'c_ne': [-1, 2], 'c_e': [-1, 1], 'c_se': [-1, 0], \n",
    "                       'c_s': [-2, 0], 'c_sw': [-3, 0], 'c_w': [-3, 1], 'c_nw': [-3, 2]\n",
    "                      }\n",
    "    # Convert lists to np.arrays\n",
    "    for key in word_to_vec_map.keys():\n",
    "        word_to_vec_map[key] = np.array(word_to_vec_map[key])\n",
    "        \n",
    "    # Training set. Sentences composed of a_* words will be of class 0 and sentences composed of c_* words will be of class 1\n",
    "    X = np.asarray(['a a_s synonym_of_a a_n c_sw', 'a a_s a_n c_sw', 'a_s  a a_n', 'synonym_of_a a a_s a_n c_sw', \" a_s a_n\",\n",
    "                    \" a a_s a_n c \", \" a_n  a c c c_e\",\n",
    "                   'c c_nw c_n c c_ne', 'c_e c c_se c_s', 'c_nw c a_s c_e c_e', 'c_e a_nw c_sw', 'c_sw c c_ne c_ne'])\n",
    "    \n",
    "    Y = np.asarray([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "    \n",
    "    np.random.seed(10)\n",
    "    pred, W, b = model(X, Y, word_to_vec_map, 0.0025, 110)\n",
    "    \n",
    "    assert W.shape == (2, 2), \"W must be of shape 2 x 2\"\n",
    "    assert np.allclose(pred.transpose(), Y), \"Model must give a perfect accuracy\"\n",
    "    assert np.allclose(b[0], -1 * b[1]), \"b should be symmetric in this example\"\n",
    "    \n",
    "    print(\"\\033[92mAll tests passed!\")\n",
    "    \n",
    "model_test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132,)\n",
      "(132,)\n",
      "(132, 5)\n",
      "never talk to me again\n",
      "<class 'numpy.ndarray'>\n",
      "(20,)\n",
      "(20,)\n",
      "(132, 5)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
    "print(X_train[0])\n",
    "print(type(X_train))\n",
    "Y = np.asarray([5, 0, 0, 5, 4, 4, 4, 6, 6, 4, 1, 1, 5, 6, 6, 3, 6, 3, 4, 4])\n",
    "print(Y.shape)\n",
    "\n",
    "X = np.asarray(['I am going to the bar tonight', 'I love you', 'miss you my dear',\n",
    " 'Lets go party and have drinks','Congrats on the new job','Congratulations',\n",
    " 'I am so happy for you', 'Why are you feeling bad', 'What is wrong with you',\n",
    " 'You totally deserve this prize', 'Let us go play football',\n",
    " 'Are you down for football this afternoon', 'Work hard play harder',\n",
    " 'It is surprising how people can be dumb sometimes',\n",
    " 'I am very disappointed','It is the best day in my life',\n",
    " 'I think I will end up alone','My life is so boring','Good job',\n",
    " 'Great so awesome'])\n",
    "\n",
    "print(X.shape)\n",
    "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 1.9520498812810072\n",
      "Accuracy: 0.3484848484848485\n",
      "Epoch: 10 --- cost = 1.0040987758894053\n",
      "Accuracy: 0.7272727272727273\n",
      "[[3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [0.]\n",
      " [4.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [0.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Accuracy: 0.803030303030303\n",
      "Test set:\n",
      "Accuracy: 0.7321428571428571\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
    "print('Test set:')\n",
    "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(sentence, W=W, b=b, word_to_vec_map=word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Given X (sentences) and Y (emoji indices), predict emojis and compute the accuracy of your model over the given set.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data containing sentences, numpy array of shape (m, None)\n",
    "    Y -- labels, containing index of the label emoji, numpy array of shape (m, 1)\n",
    "    \n",
    "    Returns:\n",
    "    pred -- numpy array of shape (m, 1) with your predictions\n",
    "    \"\"\"\n",
    "\n",
    "    any_word = list(word_to_vec_map.keys())[0]\n",
    "    # number of classes  \n",
    "    n_h = word_to_vec_map[any_word].shape[0] \n",
    "        \n",
    "    # Split jth test example (sentence) into list of lower case words\n",
    "    words = sentence.lower().split()\n",
    "\n",
    "    # Average words' vectors\n",
    "    avg = np.zeros((n_h,))\n",
    "    count = 0\n",
    "    for w in words:\n",
    "        if w in word_to_vec_map:\n",
    "            avg += word_to_vec_map[w]\n",
    "            count += 1\n",
    "\n",
    "    if count > 0:\n",
    "        avg = avg / count\n",
    "\n",
    "    # Forward propagation\n",
    "    Z = np.dot(W, avg) + b\n",
    "    A = softmax(Z)\n",
    "    pred = np.argmax(A)\n",
    "        \n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':smile:'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_emoji(int(predict_single(\"He is amazing\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56,)\n",
      "           ❤️    ⚾    :smile:    :disappointed:   🍴\n",
      "Predicted  0.0  1.0  2.0  3.0  4.0  All\n",
      "Actual                                 \n",
      "0            4    1    0    2    0    7\n",
      "1            0    8    0    0    0    8\n",
      "2            2    0   14    2    0   18\n",
      "3            2    0    3   11    0   16\n",
      "4            0    0    1    2    4    7\n",
      "All          8    9   18   17    4   56\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(Y_test.shape)\n",
    "print('           '+ label_to_emoji(0)+ '    ' + label_to_emoji(1) + '    ' +  label_to_emoji(2)+ '    ' + label_to_emoji(3)+'   ' + label_to_emoji(4))\n",
    "print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
    "plot_confusion_matrix(Y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7c3a54a9154ab0113f48d48b1eef31653434e6001d3e1c35c5f0767e2012325"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
