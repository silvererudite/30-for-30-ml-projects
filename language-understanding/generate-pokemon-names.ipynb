{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport pprint\nimport copy\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-20T14:11:15.886351Z","iopub.execute_input":"2022-09-20T14:11:15.886782Z","iopub.status.idle":"2022-09-20T14:11:15.893689Z","shell.execute_reply.started":"2022-09-20T14:11:15.886748Z","shell.execute_reply":"2022-09-20T14:11:15.892141Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"data = open('../input/pokemon-names/pokemon_names.txt', 'r').read()\ndata= data.lower()\nchars = list(set(data))\ndata_size, vocab_size = len(data), len(chars)\nprint('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:11:15.896801Z","iopub.execute_input":"2022-09-20T14:11:15.897804Z","iopub.status.idle":"2022-09-20T14:11:15.916432Z","shell.execute_reply.started":"2022-09-20T14:11:15.897768Z","shell.execute_reply":"2022-09-20T14:11:15.914750Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"There are 6839 total characters and 36 unique characters in your data.\n","output_type":"stream"}]},{"cell_type":"code","source":"chars = sorted(chars)\nprint(chars)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:11:15.919178Z","iopub.execute_input":"2022-09-20T14:11:15.919706Z","iopub.status.idle":"2022-09-20T14:11:15.927302Z","shell.execute_reply.started":"2022-09-20T14:11:15.919656Z","shell.execute_reply":"2022-09-20T14:11:15.926223Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"['\\n', ' ', \"'\", '-', '.', '2', ':', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'é', '♀', '♂']\n","output_type":"stream"}]},{"cell_type":"code","source":"char_to_ix = { ch:i for i,ch in enumerate(chars) }\nix_to_char = { i:ch for i,ch in enumerate(chars) }\npp = pprint.PrettyPrinter(indent=4)\npp.pprint(ix_to_char)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:11:15.929023Z","iopub.execute_input":"2022-09-20T14:11:15.929500Z","iopub.status.idle":"2022-09-20T14:11:15.940102Z","shell.execute_reply.started":"2022-09-20T14:11:15.929468Z","shell.execute_reply":"2022-09-20T14:11:15.938759Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"{   0: '\\n',\n    1: ' ',\n    2: \"'\",\n    3: '-',\n    4: '.',\n    5: '2',\n    6: ':',\n    7: 'a',\n    8: 'b',\n    9: 'c',\n    10: 'd',\n    11: 'e',\n    12: 'f',\n    13: 'g',\n    14: 'h',\n    15: 'i',\n    16: 'j',\n    17: 'k',\n    18: 'l',\n    19: 'm',\n    20: 'n',\n    21: 'o',\n    22: 'p',\n    23: 'q',\n    24: 'r',\n    25: 's',\n    26: 't',\n    27: 'u',\n    28: 'v',\n    29: 'w',\n    30: 'x',\n    31: 'y',\n    32: 'z',\n    33: 'é',\n    34: '♀',\n    35: '♂'}\n","output_type":"stream"}]},{"cell_type":"code","source":"def clip(gradients, maxValue):\n    '''\n    Clips the gradients' values between minimum and maximum.\n    \n    Arguments:\n    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n    \n    Returns: \n    gradients -- a dictionary with the clipped gradients.\n    '''\n    gradients = copy.deepcopy(gradients)\n    \n    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n    for gradient in gradients:\n        np.clip(gradients[gradient], -maxValue, maxValue, out = gradients[gradient])\n    \n    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n    \n    return gradients","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:11:15.944621Z","iopub.execute_input":"2022-09-20T14:11:15.945577Z","iopub.status.idle":"2022-09-20T14:11:15.954544Z","shell.execute_reply.started":"2022-09-20T14:11:15.945541Z","shell.execute_reply":"2022-09-20T14:11:15.952918Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def clip_test(target, mValue):\n    print(f\"\\nGradients for mValue={mValue}\")\n    np.random.seed(3)\n    dWax = np.random.randn(5, 3) * 10\n    dWaa = np.random.randn(5, 5) * 10\n    dWya = np.random.randn(2, 5) * 10\n    db = np.random.randn(5, 1) * 10\n    dby = np.random.randn(2, 1) * 10\n    gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n\n    gradients2 = target(gradients, mValue)\n    print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients2[\"dWaa\"][1][2])\n    print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients2[\"dWax\"][3][1])\n    print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients2[\"dWya\"][1][2])\n    print(\"gradients[\\\"db\\\"][4] =\", gradients2[\"db\"][4])\n    print(\"gradients[\\\"dby\\\"][1] =\", gradients2[\"dby\"][1])\n    \n    for grad in gradients2.keys():\n        valuei = gradients[grad]\n        valuef = gradients2[grad]\n        mink = np.min(valuef)\n        maxk = np.max(valuef)\n        assert mink >= -abs(mValue), f\"Problem with {grad}. Set a_min to -mValue in the np.clip call\"\n        assert maxk <= abs(mValue), f\"Problem with {grad}.Set a_max to mValue in the np.clip call\"\n        index_not_clipped = np.logical_and(valuei <= mValue, valuei >= -mValue)\n        assert np.all(valuei[index_not_clipped] == valuef[index_not_clipped]), f\" Problem with {grad}. Some values that should not have changed, changed during the clipping process.\"\n    \n    print(\"\\033[92mAll tests passed!\\x1b[0m\")\n    \nclip_test(clip, 10)\nclip_test(clip, 5)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:11:15.956875Z","iopub.execute_input":"2022-09-20T14:11:15.957721Z","iopub.status.idle":"2022-09-20T14:11:15.976182Z","shell.execute_reply.started":"2022-09-20T14:11:15.957674Z","shell.execute_reply":"2022-09-20T14:11:15.974733Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"\nGradients for mValue=10\ngradients[\"dWaa\"][1][2] = 10.0\ngradients[\"dWax\"][3][1] = -10.0\ngradients[\"dWya\"][1][2] = 0.2971381536101662\ngradients[\"db\"][4] = [10.]\ngradients[\"dby\"][1] = [8.45833407]\n\u001b[92mAll tests passed!\u001b[0m\n\nGradients for mValue=5\ngradients[\"dWaa\"][1][2] = 5.0\ngradients[\"dWax\"][3][1] = -5.0\ngradients[\"dWya\"][1][2] = 0.2971381536101662\ngradients[\"db\"][4] = [5.]\ngradients[\"dby\"][1] = [5.]\n\u001b[92mAll tests passed!\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"def softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:11:15.977393Z","iopub.execute_input":"2022-09-20T14:11:15.978220Z","iopub.status.idle":"2022-09-20T14:11:15.990536Z","shell.execute_reply.started":"2022-09-20T14:11:15.978187Z","shell.execute_reply":"2022-09-20T14:11:15.989117Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"def sample(parameters, char_to_ix, seed):\n    \"\"\"\n    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n\n    Arguments:\n    parameters -- Python dictionary containing the parameters Waa, Wax, Wya, by, and b. \n    char_to_ix -- Python dictionary mapping each character to an index.\n    seed -- Used for grading purposes. Do not worry about it.\n\n    Returns:\n    indices -- A list of length n containing the indices of the sampled characters.\n    \"\"\"\n\n    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n    vocab_size = by.shape[0]\n    n_a = Waa.shape[1]\n    \n    x = np.zeros((vocab_size, 1))\n    a_prev = np.zeros((n_a, 1))\n    \n    indices = []\n    idx = -1\n    \n    counter = 0\n    newline_character = char_to_ix['\\n']\n    \n    while (idx != newline_character and counter != 50):\n        a = np.tanh(np.dot(Wax,x) + np.dot(Waa,a_prev) + b)\n        z = np.dot(Wya,a) + by\n        y = softmax(z)\n        \n        np.random.seed(counter + seed) \n        idx = np.random.choice(range(len(y)), p = np.squeeze(y) )\n        indices.append(idx)\n        \n        x = np.zeros((vocab_size,1))\n        x[idx] = 1\n        \n        a_prev = a\n        seed += 1\n\n        counter +=1\n    if (counter == 50):\n        indices.append(char_to_ix['\\n'])\n    \n    return indices\n    ","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:11:15.994450Z","iopub.execute_input":"2022-09-20T14:11:15.994828Z","iopub.status.idle":"2022-09-20T14:11:16.007990Z","shell.execute_reply.started":"2022-09-20T14:11:15.994795Z","shell.execute_reply":"2022-09-20T14:11:16.006279Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def sample_test(target):\n    np.random.seed(24)\n    _, n_a = 20, 100\n    Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n    b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n\n\n    indices = target(parameters, char_to_ix, 0)\n    print(\"Sampling:\")\n    print(\"list of sampled indices:\\n\", indices)\n    print(\"list of sampled characters:\\n\", [ix_to_char[i] for i in indices])\n    \n    assert len(indices) < 52, \"Indices lenght must be smaller than 52\"\n    assert indices[-1] == char_to_ix['\\n'], \"All samples must end with \\\\n\"\n    assert min(indices) >= 0 and max(indices) < len(char_to_ix), f\"Sampled indexes must be between 0 and len(char_to_ix)={len(char_to_ix)}\"\n    assert np.allclose(indices[0:6], [23, 16, 26, 26, 24, 3]), \"Wrong values\"\n    \n    print(\"\\033[92mAll tests passed!\")\n\n#sample_test(sample)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:11:27.777803Z","iopub.execute_input":"2022-09-20T14:11:27.778239Z","iopub.status.idle":"2022-09-20T14:11:27.791143Z","shell.execute_reply.started":"2022-09-20T14:11:27.778204Z","shell.execute_reply":"2022-09-20T14:11:27.789774Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def rnn_step_forward(parameters, a_prev, x):\n    \n    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b) # hidden state\n    p_t = softmax(np.dot(Wya, a_next) + by) # unnormalized log probabilities for next chars # probabilities for next chars \n    \n    return a_next, p_t\n\ndef rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n    \n    gradients['dWya'] += np.dot(dy, a.T)\n    gradients['dby'] += dy\n    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] # backprop into h\n    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n    gradients['db'] += daraw\n    gradients['dWax'] += np.dot(daraw, x.T)\n    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n    return gradients\n\ndef update_parameters(parameters, gradients, lr):\n\n    parameters['Wax'] += -lr * gradients['dWax']\n    parameters['Waa'] += -lr * gradients['dWaa']\n    parameters['Wya'] += -lr * gradients['dWya']\n    parameters['b']  += -lr * gradients['db']\n    parameters['by']  += -lr * gradients['dby']\n    return parameters\n\ndef rnn_forward(X, Y, a0, parameters, vocab_size = 36):\n    \n    # Initialize x, a and y_hat as empty dictionaries\n    x, a, y_hat = {}, {}, {}\n    \n    a[-1] = np.copy(a0)\n    \n    # initialize your loss to 0\n    loss = 0\n    \n    for t in range(len(X)):\n        \n        # Set x[t] to be the one-hot vector representation of the t'th character in X.\n        # if X[t] == None, we just have x[t]=0. This is used to set the input for the first timestep to the zero vector. \n        x[t] = np.zeros((vocab_size,1)) \n        if (X[t] != None):\n            x[t][X[t]] = 1\n        \n        # Run one step forward of the RNN\n        a[t], y_hat[t] = rnn_step_forward(parameters, a[t-1], x[t])\n        \n        # Update the loss by substracting the cross-entropy term of this time-step from it.\n        loss -= np.log(y_hat[t][Y[t],0])\n        \n    cache = (y_hat, a, x)\n        \n    return loss, cache\n\ndef rnn_backward(X, Y, parameters, cache):\n    # Initialize gradients as an empty dictionary\n    gradients = {}\n    \n    # Retrieve from cache and parameters\n    (y_hat, a, x) = cache\n    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n    \n    # each one should be initialized to zeros of the same dimension as its corresponding parameter\n    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n    gradients['db'], gradients['dby'] = np.zeros_like(b), np.zeros_like(by)\n    gradients['da_next'] = np.zeros_like(a[0])\n    \n    ### START CODE HERE ###\n    # Backpropagate through time\n    for t in reversed(range(len(X))):\n        dy = np.copy(y_hat[t])\n        dy[Y[t]] -= 1\n        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t-1])\n    ### END CODE HERE ###\n    \n    return gradients, a","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:11:27.793821Z","iopub.execute_input":"2022-09-20T14:11:27.794474Z","iopub.status.idle":"2022-09-20T14:11:27.815360Z","shell.execute_reply.started":"2022-09-20T14:11:27.794425Z","shell.execute_reply":"2022-09-20T14:11:27.813590Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n    \"\"\"\n    Execute one step of the optimization to train the model.\n    \n    Arguments:\n    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n    Y -- list of integers, exactly the same as X but shifted one index to the left.\n    a_prev -- previous hidden state.\n    parameters -- python dictionary containing:\n                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n                        b --  Bias, numpy array of shape (n_a, 1)\n                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n    learning_rate -- learning rate for the model.\n    \n    Returns:\n    loss -- value of the loss function (cross-entropy)\n    gradients -- python dictionary containing:\n                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n                        db -- Gradients of bias vector, of shape (n_a, 1)\n                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n    \"\"\"\n    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n    gradients, a = rnn_backward(X, Y, a_prev, cache)\n    gradients = clip(gradients, 5)\n    parameters = update_parameters(parameters, gradients, learning_rate)\n    return loss, gradients, a[len(X)-1]","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:14:51.967748Z","iopub.execute_input":"2022-09-20T14:14:51.968204Z","iopub.status.idle":"2022-09-20T14:14:51.977935Z","shell.execute_reply.started":"2022-09-20T14:14:51.968170Z","shell.execute_reply":"2022-09-20T14:14:51.976347Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"def optimize_test(target):\n    np.random.seed(1)\n    vocab_size, n_a = 36, 100\n    a_prev = np.random.randn(n_a, 1)\n    Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n    b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n    X = [12, 3, 5, 11, 22, 3]\n    Y = [4, 14, 11, 22, 25, 26]\n    old_parameters = copy.deepcopy(parameters)\n    loss, gradients, a_last = target(X, Y, a_prev, parameters, learning_rate = 0.01)\n    print(\"Loss =\", loss)\n    print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n    print(\"np.argmax(gradients[\\\"dWax\\\"]) =\", np.argmax(gradients[\"dWax\"]))\n    print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n    print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n    print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])\n    print(\"a_last[4] =\", a_last[4])\n    \n    assert np.isclose(loss, 126.5039757), \"Problems with the call of the rnn_forward function\"\n    for grad in gradients.values():\n        assert np.min(grad) >= -5, \"Problems in the clip function call\"\n        assert np.max(grad) <= 5, \"Problems in the clip function call\"\n    assert np.allclose(gradients['dWaa'][1, 2], 0.1947093), \"Unexpected gradients. Check the rnn_backward call\"\n    assert np.allclose(gradients['dWya'][1, 2], -0.007773876), \"Unexpected gradients. Check the rnn_backward call\"\n    assert not np.allclose(parameters['Wya'], old_parameters['Wya']), \"parameters were not updated\"\n    \n    print(\"\\033[92mAll tests passed!\")\n\n#optimize_test(optimize)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:13:55.390348Z","iopub.execute_input":"2022-09-20T14:13:55.390783Z","iopub.status.idle":"2022-09-20T14:13:55.493629Z","shell.execute_reply.started":"2022-09-20T14:13:55.390749Z","shell.execute_reply":"2022-09-20T14:13:55.491489Z"},"trusted":true},"execution_count":56,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/2563601917.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\033[92mAll tests passed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0moptimize_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_17/2563601917.py\u001b[0m in \u001b[0;36moptimize_test\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m26\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mold_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gradients[\\\"dWaa\\\"][1][2] =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dWaa\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/3544105279.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(X, Y, a_prev, parameters, learning_rate)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \"\"\"\n\u001b[1;32m     27\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/2252647885.py\u001b[0m in \u001b[0;36mrnn_backward\u001b[0;34m(X, Y, parameters, cache)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Retrieve from cache and parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mWaa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWya\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Waa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Wax'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Wya'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'by'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# each one should be initialized to zeros of the same dimension as its corresponding parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"],"ename":"IndexError","evalue":"only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices","output_type":"error"}]},{"cell_type":"code","source":"def smooth(loss, cur_loss):\n    return loss * 0.999 + cur_loss * 0.001\n\ndef print_sample(sample_ix, ix_to_char):\n    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n    txt = txt[0].upper() + txt[1:]  # capitalize first character \n    print ('%s' % (txt, ), end='')\n    \n\ndef get_sample(sample_ix, ix_to_char):\n    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n    txt = txt[0].upper() + txt[1:]  # capitalize first character \n    return txt\n\ndef get_initial_loss(vocab_size, seq_length):\n    return -np.log(1.0/vocab_size)*seq_length\n\ndef initialize_parameters(n_a, n_x, n_y):\n    \"\"\"\n    Initialize parameters with small random values\n    \n    Returns:\n    parameters -- python dictionary containing:\n                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n                        b --  Bias, numpy array of shape (n_a, 1)\n                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n    \"\"\"\n    np.random.seed(1)\n    Wax = np.random.randn(n_a, n_x)*0.01 # input to hidden\n    Waa = np.random.randn(n_a, n_a)*0.01 # hidden to hidden\n    Wya = np.random.randn(n_y, n_a)*0.01 # hidden to output\n    b = np.zeros((n_a, 1)) # hidden bias\n    by = np.zeros((n_y, 1)) # output bias\n    \n    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b,\"by\": by}\n    \n    return parameters","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:11:27.855898Z","iopub.execute_input":"2022-09-20T14:11:27.856967Z","iopub.status.idle":"2022-09-20T14:11:27.870925Z","shell.execute_reply.started":"2022-09-20T14:11:27.856915Z","shell.execute_reply":"2022-09-20T14:11:27.869359Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def model(data_x, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 36, verbose = False):\n    \"\"\"\n    Trains the model and generates dinosaur names. \n    \n    Arguments:\n    data_x -- text corpus, divided in words\n    ix_to_char -- dictionary that maps the index to a character\n    char_to_ix -- dictionary that maps a character to an index\n    num_iterations -- number of iterations to train the model for\n    n_a -- number of units of the RNN cell\n    dino_names -- number of dinosaur names you want to sample at each iteration. \n    vocab_size -- number of unique characters found in the text (size of the vocabulary)\n    \n    Returns:\n    parameters -- learned parameters\n    \"\"\"\n    \n    # Retrieve n_x and n_y from vocab_size\n    n_x, n_y = vocab_size, vocab_size\n    \n    # Initialize parameters\n    parameters = initialize_parameters(n_a, n_x, n_y)\n    \n    # Initialize loss (this is required because we want to smooth our loss)\n    loss = get_initial_loss(vocab_size, dino_names)\n    \n    # Build list of all dinosaur names (training examples).\n    examples = [x.strip() for x in data_x]\n    \n    # Shuffle list of all dinosaur names\n    np.random.seed(0)\n    np.random.shuffle(examples)\n    \n    # Initialize the hidden state of your LSTM\n    a_prev = np.zeros((n_a, 1))\n    \n    # for grading purposes\n    last_dino_name = \"abc\"\n    \n    # Optimization loop\n    for j in range(num_iterations):\n        \n        ### START CODE HERE ###\n        \n        # Set the index `idx` (see instructions above)\n        idx = j%len(examples)\n        \n        # Set the input X (see instructions above)\n        single_example_chars = examples[idx]\n        single_example_ix = [char_to_ix[c] for c in single_example_chars]\n\n        # if X[t] == None, we just have x[t]=0. This is used to set the input for the first timestep to the zero vector. \n        X = [None] + single_example_ix\n        \n        # Set the labels Y (see instructions above)\n        # The goal is to train the RNN to predict the next letter in the name\n        # So the labels are the list of characters that are one time-step ahead of the characters in the input X\n        Y = X[1:] \n        # The RNN should predict a newline at the last letter, so add ix_newline to the end of the labels\n        ix_newline = [char_to_ix[\"\\n\"]]\n        Y = Y + ix_newline\n\n        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n        # Choose a learning rate of 0.01\n        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n        \n        ### END CODE HERE ###\n        \n        # debug statements to aid in correctly forming X, Y\n        if verbose and j in [0, len(examples) -1, len(examples)]:\n            print(\"j = \" , j, \"idx = \", idx,) \n        if verbose and j in [0]:\n            #print(\"single_example =\", single_example)\n            print(\"single_example_chars\", single_example_chars)\n            print(\"single_example_ix\", single_example_ix)\n            print(\" X = \", X, \"\\n\", \"Y =       \", Y, \"\\n\")\n        \n        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n        loss = smooth(loss, curr_loss)\n\n        # Every 1000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n        if j % 1000 == 0:\n            \n            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n            \n            # The number of dinosaur names to print\n            seed = 0\n            for name in range(dino_names):\n                \n                # Sample indices and print them\n                sampled_indices = sample(parameters, char_to_ix, seed)\n                last_dino_name = get_sample(sampled_indices, ix_to_char)\n                print(last_dino_name.replace('\\n', ''))\n                \n                seed += 1  # To get the same result (for grading purposes), increment the seed by one. \n      \n            print('\\n')\n        \n    return parameters, last_dino_name","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:11:27.873632Z","iopub.execute_input":"2022-09-20T14:11:27.874215Z","iopub.status.idle":"2022-09-20T14:11:27.892200Z","shell.execute_reply.started":"2022-09-20T14:11:27.874164Z","shell.execute_reply":"2022-09-20T14:11:27.890529Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"parameters, last_name = model(data.split(\"\\n\"), ix_to_char, char_to_ix, 22001, verbose = True)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:15:04.872703Z","iopub.execute_input":"2022-09-20T14:15:04.873153Z","iopub.status.idle":"2022-09-20T14:15:04.911268Z","shell.execute_reply.started":"2022-09-20T14:15:04.873118Z","shell.execute_reply":"2022-09-20T14:15:04.909471Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":59,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/3205522155.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix_to_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m22001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_17/89060023.py\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(data_x, ix_to_char, char_to_ix, num_iterations, n_a, dino_names, vocab_size, verbose)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# Choose a learning rate of 0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mcurr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/3544105279.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(X, Y, a_prev, parameters, learning_rate)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \"\"\"\n\u001b[1;32m     27\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/2252647885.py\u001b[0m in \u001b[0;36mrnn_backward\u001b[0;34m(X, Y, parameters, cache)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Retrieve from cache and parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mWaa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWya\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Waa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Wax'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Wya'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'by'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# each one should be initialized to zeros of the same dimension as its corresponding parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"],"ename":"IndexError","evalue":"only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}